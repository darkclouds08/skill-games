[{ "id": 1, "question": "What is the primary characteristic of supervised learning?", "options": ["Learning without labeled data", "Learning with input-output pairs", "Clustering similar data points", "Reducing data dimensionality"], "answer": "Learning with input-output pairs", "difficulty": "easy" },
{ "id": 2, "question": "In a neural network for classification, what does the output layer typically represent?", "options": ["Input features", "Hidden representations", "Class probabilities", "Training errors"], "answer": "Class probabilities", "difficulty": "easy" },
{ "id": 3, "question": "Which activation function is commonly used in the output layer of a binary classification neural network?", "options": ["ReLU", "Sigmoid", "Tanh", "Linear"], "answer": "Sigmoid", "difficulty": "easy" },
{ "id": 4, "question": "What is the purpose of a loss function in supervised learning?", "options": ["To increase model complexity", "To measure prediction errors", "To add more layers", "To normalize inputs"], "answer": "To measure prediction errors", "difficulty": "easy" },
{ "id": 5, "question": "Which of the following is a supervised learning algorithm?", "options": ["K-Means", "Linear Regression", "DBSCAN", "PCA"], "answer": "Linear Regression", "difficulty": "easy" },
{ "id": 6, "question": "What type of data is required for supervised learning?", "options": ["Unlabeled data only", "Labeled training data", "Raw unprocessed data", "Categorical data only"], "answer": "Labeled training data", "difficulty": "easy" },
{ "id": 7, "question": "In a multi-class classification problem, which activation function is typically used in the output layer?", "options": ["ReLU", "Sigmoid", "Softmax", "Tanh"], "answer": "Softmax", "difficulty": "easy" },
{ "id": 8, "question": "What does a neuron in a neural network do?", "options": ["Stores data permanently", "Computes weighted sum and applies activation", "Reduces data size", "Sorts input values"], "answer": "Computes weighted sum and applies activation", "difficulty": "easy" },
{ "id": 9, "question": "Which of the following is NOT a component of a basic neural network?", "options": ["Input layer", "Hidden layer", "Output layer", "Clustering layer"], "answer": "Clustering layer", "difficulty": "easy" },
{ "id": 10, "question": "What is the role of weights in a neural network?", "options": ["To store input data", "To determine connection strength between neurons", "To count the number of layers", "To measure accuracy"], "answer": "To determine connection strength between neurons", "difficulty": "easy" },
{ "id": 11, "question": "Which loss function is commonly used for binary classification?", "options": ["Mean Squared Error", "Binary Cross-entropy", "Hinge Loss", "Huber Loss"], "answer": "Binary Cross-entropy", "difficulty": "easy" },
{ "id": 12, "question": "What is backpropagation used for in neural networks?", "options": ["Forward data flow", "Weight initialization", "Error propagation and weight updates", "Data preprocessing"], "answer": "Error propagation and weight updates", "difficulty": "easy" },
{ "id": 13, "question": "In supervised learning, what is the training set?", "options": ["Data used to test final model", "Data used to train the model", "Data used for validation", "Unlabeled data"], "answer": "Data used to train the model", "difficulty": "easy" },
{ "id": 14, "question": "What does the term 'epoch' mean in neural network training?", "options": ["One forward pass", "One backward pass", "One complete pass through entire training dataset", "One weight update"], "answer": "One complete pass through entire training dataset", "difficulty": "easy" },
{ "id": 15, "question": "Which of the following is a classification task?", "options": ["Predicting house prices", "Email spam detection", "Stock price forecasting", "Temperature prediction"], "answer": "Email spam detection", "difficulty": "easy" },
{ "id": 16, "question": "What is the purpose of an activation function?", "options": ["To store weights", "To introduce non-linearity", "To reduce overfitting", "To normalize data"], "answer": "To introduce non-linearity", "difficulty": "easy" },
{ "id": 17, "question": "In a neural network, what connects neurons between layers?", "options": ["Biases", "Weights", "Activations", "Gradients"], "answer": "Weights", "difficulty": "easy" },
{ "id": 18, "question": "Which of the following describes a perceptron?", "options": ["Multi-layer neural network", "Single-layer neural network", "Unsupervised learning algorithm", "Data preprocessing technique"], "answer": "Single-layer neural network", "difficulty": "easy" },
{ "id": 19, "question": "What is the main goal of a classification algorithm?", "options": ["Predict continuous values", "Assign data points to categories", "Reduce data dimensions", "Find data clusters"], "answer": "Assign data points to categories", "difficulty": "easy" },
{ "id": 20, "question": "Which type of neural network architecture is feed-forward?", "options": ["Recurrent Neural Network", "Long Short-Term Memory", "Multi-layer Perceptron", "Convolutional Neural Network"], "answer": "Multi-layer Perceptron", "difficulty": "easy" },
{ "id": 21, "question": "What is gradient descent used for in neural networks?", "options": ["Data preprocessing", "Weight initialization", "Optimizing weights to minimize loss", "Feature selection"], "answer": "Optimizing weights to minimize loss", "difficulty": "easy" },
{ "id": 22, "question": "In supervised learning, what is a label?", "options": ["Input feature", "Model parameter", "Known output/target value", "Prediction error"], "answer": "Known output/target value", "difficulty": "easy" },
{ "id": 23, "question": "Which of the following is true about the ReLU activation function?", "options": ["Outputs values between -1 and 1", "Outputs values between 0 and 1", "Outputs zero for negative inputs", "Always outputs positive values"], "answer": "Outputs zero for negative inputs", "difficulty": "easy" },
{ "id": 24, "question": "What does MLP stand for in neural networks?", "options": ["Multi-Layer Perceptron", "Maximum Likelihood Prediction", "Multiple Linear Programming", "Machine Learning Protocol"], "answer": "Multi-Layer Perceptron", "difficulty": "easy" },
{ "id": 25, "question": "In a binary classification problem, how many output neurons are typically used?", "options": ["1", "2", "3", "4"], "answer": "1", "difficulty": "easy" },
{ "id": 26, "question": "What is overfitting in machine learning?", "options": ["Model performs well on both training and test data", "Model performs poorly on training data", "Model memorizes training data but fails on new data", "Model has too few parameters"], "answer": "Model memorizes training data but fails on new data", "difficulty": "easy" },
{ "id": 27, "question": "Which metric is commonly used to evaluate classification models?", "options": ["Mean Squared Error", "R-squared", "Accuracy", "Mean Absolute Error"], "answer": "Accuracy", "difficulty": "easy" },
{ "id": 28, "question": "What is the purpose of a bias term in a neuron?", "options": ["To store weights", "To shift the activation function", "To prevent overfitting", "To normalize inputs"], "answer": "To shift the activation function", "difficulty": "easy" },
{ "id": 29, "question": "Which of the following is a characteristic of deep learning?", "options": ["Uses only linear functions", "Has many hidden layers", "Requires no training data", "Works only with images"], "answer": "Has many hidden layers", "difficulty": "easy" },
{ "id": 30, "question": "What is the input layer of a neural network?", "options": ["Layer that produces final output", "Layer that receives raw input data", "Layer that computes gradients", "Layer that stores weights"], "answer": "Layer that receives raw input data", "difficulty": "easy" },
{ "id": 31, "question": "In supervised learning, what is feature engineering?", "options": ["Creating new models", "Selecting and transforming input variables", "Tuning hyperparameters", "Evaluating model performance"], "answer": "Selecting and transforming input variables", "difficulty": "easy" },
{ "id": 32, "question": "What does SGD stand for in neural network optimization?", "options": ["Structured Gradient Descent", "Stochastic Gradient Descent", "Standard Gradient Distribution", "Sequential Gradient Development"], "answer": "Stochastic Gradient Descent", "difficulty": "easy" },
{ "id": 33, "question": "Which of the following is NOT typically a step in supervised learning?", "options": ["Data preparation", "Model training", "Model evaluation", "Data clustering"], "answer": "Data clustering", "difficulty": "easy" },
{ "id": 34, "question": "What is the purpose of the validation set in machine learning?", "options": ["To train the model", "To test final model performance", "To tune hyperparameters", "To store backup data"], "answer": "To tune hyperparameters", "difficulty": "easy" },
{ "id": 35, "question": "In neural networks, what is forward propagation?", "options": ["Updating weights backward", "Computing output from input", "Error calculation", "Weight initialization"], "answer": "Computing output from input", "difficulty": "easy" },
{ "id": 36, "question": "Which activation function can output negative values?", "options": ["ReLU", "Sigmoid", "Tanh", "Softmax"], "answer": "Tanh", "difficulty": "easy" },
{ "id": 37, "question": "What is a hyperparameter in machine learning?", "options": ["Parameter learned during training", "Parameter set before training", "Output of the model", "Input feature"], "answer": "Parameter set before training", "difficulty": "easy" },
{ "id": 38, "question": "Which loss function is used for multi-class classification?", "options": ["Binary cross-entropy", "Categorical cross-entropy", "Mean squared error", "Hinge loss"], "answer": "Categorical cross-entropy", "difficulty": "easy" },
{ "id": 39, "question": "What is the purpose of normalization in neural networks?", "options": ["To increase model complexity", "To scale input features appropriately", "To add more neurons", "To reduce training time"], "answer": "To scale input features appropriately", "difficulty": "easy" },
{ "id": 40, "question": "In a confusion matrix for binary classification, what does a true positive represent?", "options": ["Incorrectly classified negative", "Correctly classified positive", "Incorrectly classified positive", "Correctly classified negative"], "answer": "Correctly classified positive", "difficulty": "easy" },
{ "id": 41, "question": "What is the learning rate in neural network training?", "options": ["Rate of data processing", "Step size for weight updates", "Number of training examples", "Model accuracy improvement"], "answer": "Step size for weight updates", "difficulty": "easy" },
{ "id": 42, "question": "Which of the following is a regularization technique?", "options": ["Increasing learning rate", "Adding more layers", "Dropout", "Using more training data"], "answer": "Dropout", "difficulty": "easy" },
{ "id": 43, "question": "What does one-hot encoding do?", "options": ["Reduces data size", "Converts categorical variables to binary vectors", "Normalizes continuous variables", "Removes outliers"], "answer": "Converts categorical variables to binary vectors", "difficulty": "easy" },
{ "id": 44, "question": "In neural networks, what is the purpose of hidden layers?", "options": ["To receive input data", "To produce final output", "To learn complex patterns and representations", "To store training data"], "answer": "To learn complex patterns and representations", "difficulty": "easy" },
{ "id": 45, "question": "Which evaluation metric measures the proportion of correct predictions?", "options": ["Precision", "Recall", "Accuracy", "F1-score"], "answer": "Accuracy", "difficulty": "easy" },
{ "id": 46, "question": "What is batch size in neural network training?", "options": ["Total number of training examples", "Number of examples processed before weight update", "Number of epochs", "Number of layers"], "answer": "Number of examples processed before weight update", "difficulty": "easy" },
{ "id": 47, "question": "Which of the following describes underfitting?", "options": ["Model performs well on training data", "Model is too complex", "Model is too simple and performs poorly", "Model memorizes training data"], "answer": "Model is too simple and performs poorly", "difficulty": "easy" },
{ "id": 48, "question": "What is the purpose of cross-validation?", "options": ["To increase training data", "To assess model generalization", "To reduce overfitting directly", "To speed up training"], "answer": "To assess model generalization", "difficulty": "easy" },
{ "id": 49, "question": "In a neural network, what determines the number of output neurons for classification?", "options": ["Number of input features", "Number of hidden layers", "Number of classes to predict", "Size of training data"], "answer": "Number of classes to predict", "difficulty": "easy" },
{ "id": 50, "question": "What is the vanishing gradient problem?", "options": ["Gradients become too large", "Gradients become very small in deep networks", "Weights grow exponentially", "Loss function increases"], "answer": "Gradients become very small in deep networks", "difficulty": "easy" },
{ "id": 51, "question": "Which optimization algorithm is commonly used for training neural networks?", "options": ["Linear search", "Adam", "Binary search", "Bubble sort"], "answer": "Adam", "difficulty": "easy" },
{ "id": 52, "question": "What is precision in classification evaluation?", "options": ["Total correct predictions", "True positives / (True positives + False positives)", "True positives / (True positives + False negatives)", "Number of classes"], "answer": "True positives / (True positives + False positives)", "difficulty": "easy" },
{ "id": 53, "question": "What is recall in classification evaluation?", "options": ["Total correct predictions", "True positives / (True positives + False positives)", "True positives / (True positives + False negatives)", "Number of features"], "answer": "True positives / (True positives + False negatives)", "difficulty": "easy" },
{ "id": 54, "question": "Which of the following is true about the sigmoid function?", "options": ["Outputs values between -1 and 1", "Outputs values between 0 and 1", "Always outputs positive values", "Can output any real number"], "answer": "Outputs values between 0 and 1", "difficulty": "easy" },
{ "id": 55, "question": "What is early stopping in neural network training?", "options": ["Starting training early", "Stopping when validation error increases", "Stopping after fixed number of epochs", "Stopping when loss is zero"], "answer": "Stopping when validation error increases", "difficulty": "easy" },
{ "id": 56, "question": "In supervised learning, what is a feature?", "options": ["Target variable", "Input variable or attribute", "Model parameter", "Prediction error"], "answer": "Input variable or attribute", "difficulty": "easy" },
{ "id": 57, "question": "What does the term 'deep' in deep learning refer to?", "options": ["Complex algorithms", "Large datasets", "Many layers in the network", "High accuracy"], "answer": "Many layers in the network", "difficulty": "easy" },
{ "id": 58, "question": "Which of the following is a type of supervised learning?", "options": ["Clustering", "Classification", "Dimensionality reduction", "Association rules"], "answer": "Classification", "difficulty": "easy" },
{ "id": 59, "question": "What is the purpose of weight initialization in neural networks?", "options": ["To prevent overfitting", "To set starting values for training", "To normalize inputs", "To evaluate performance"], "answer": "To set starting values for training", "difficulty": "easy" },
{ "id": 60, "question": "Which metric combines precision and recall?", "options": ["Accuracy", "F1-score", "ROC curve", "Confusion matrix"], "answer": "F1-score", "difficulty": "easy" },
{ "id": 61, "question": "What is the difference between training and inference in neural networks?", "options": ["Training uses more data", "Training updates weights, inference makes predictions", "Training is faster", "No difference"], "answer": "Training updates weights, inference makes predictions", "difficulty": "easy" },
{ "id": 62, "question": "Which activation function is most commonly used in hidden layers of modern neural networks?", "options": ["Sigmoid", "Tanh", "ReLU", "Linear"], "answer": "ReLU", "difficulty": "easy" },
{ "id": 63, "question": "What is a neural network with no hidden layers called?", "options": ["Deep network", "Perceptron", "Convolutional network", "Recurrent network"], "answer": "Perceptron", "difficulty": "easy" },
{ "id": 64, "question": "In classification, what does the argmax function do?", "options": ["Calculates maximum value", "Returns index of maximum value", "Computes average", "Finds minimum value"], "answer": "Returns index of maximum value", "difficulty": "easy" },
{ "id": 65, "question": "What is the purpose of shuffling training data?", "options": ["To reduce data size", "To prevent learning data order patterns", "To increase accuracy", "To normalize features"], "answer": "To prevent learning data order patterns", "difficulty": "easy" },
{ "id": 66, "question": "Which of the following is NOT an activation function?", "options": ["ReLU", "Sigmoid", "Softmax", "Gradient"], "answer": "Gradient", "difficulty": "easy" },
{ "id": 67, "question": "What is model capacity in neural networks?", "options": ["Amount of training data", "Ability to fit complex patterns", "Number of parameters", "Training time"], "answer": "Ability to fit complex patterns", "difficulty": "easy" },
{ "id": 68, "question": "In binary classification, what threshold is commonly used with sigmoid output?", "options": ["0.3", "0.5", "0.7", "1.0"], "answer": "0.5", "difficulty": "easy" },
{ "id": 69, "question": "What is the purpose of the test set in machine learning?", "options": ["To train the model", "To validate hyperparameters", "To evaluate final model performance", "To preprocess data"], "answer": "To evaluate final model performance", "difficulty": "easy" },
{ "id": 70, "question": "Which of the following describes a fully connected layer?", "options": ["Each neuron connects to some neurons in next layer", "Each neuron connects to all neurons in next layer", "Neurons are not connected", "Only input neurons are connected"], "answer": "Each neuron connects to all neurons in next layer", "difficulty": "easy" },
{ "id": 71, "question": "What is the dying ReLU problem?", "options": ["ReLU outputs become too large", "ReLU neurons stop learning (output always 0)", "ReLU causes overfitting", "ReLU is too slow"], "answer": "ReLU neurons stop learning (output always 0)", "difficulty": "easy" },
{ "id": 72, "question": "Which loss function is appropriate for regression tasks?", "options": ["Cross-entropy", "Hinge loss", "Mean Squared Error", "Categorical loss"], "answer": "Mean Squared Error", "difficulty": "easy" },
{ "id": 73, "question": "What does the softmax function ensure about its outputs?", "options": ["All outputs are positive", "Outputs sum to 1", "Outputs are between 0 and 1", "All of the above"], "answer": "All of the above", "difficulty": "easy" },
{ "id": 74, "question": "In neural networks, what is forward pass?", "options": ["Computing gradients", "Updating weights", "Computing predictions from inputs", "Initializing weights"], "answer": "Computing predictions from inputs", "difficulty": "easy" },
{ "id": 75, "question": "What is the purpose of dropout in neural networks?", "options": ["To increase training speed", "To prevent overfitting", "To add more layers", "To normalize weights"], "answer": "To prevent overfitting", "difficulty": "easy" },
{ "id": 76, "question": "Which evaluation technique helps detect overfitting?", "options": ["Using more training data", "Comparing training and validation performance", "Increasing model complexity", "Using fewer features"], "answer": "Comparing training and validation performance", "difficulty": "easy" },
{ "id": 77, "question": "What is a mini-batch in neural network training?", "options": ["Smallest possible batch", "Subset of training data processed together", "Single training example", "Validation data"], "answer": "Subset of training data processed together", "difficulty": "early" },
{ "id": 78, "question": "Which of the following is true about neural network weights?", "options": ["They remain constant during training", "They are updated during training", "They are always positive", "They represent input data"], "answer": "They are updated during training", "difficulty": "easy" },
{ "id": 79, "question": "What is the universal approximation theorem about?", "options": ["Neural networks can approximate any continuous function", "All functions are neural networks", "Neural networks are always accurate", "Deep networks are better than shallow"], "answer": "Neural networks can approximate any continuous function", "difficulty": "easy" },
{ "id": 80, "question": "In classification, what is a decision boundary?", "options": ["Edge of training data", "Line separating different classes", "Model accuracy threshold", "Weight initialization range"], "answer": "Line separating different classes", "difficulty": "easy" },
{ "id": 81, "question": "What is gradient clipping used for?", "options": ["To prevent exploding gradients", "To increase learning rate", "To add regularization", "To normalize inputs"], "answer": "To prevent exploding gradients", "difficulty": "easy" },
{ "id": 82, "question": "Which of the following describes supervised learning feedback?", "options": ["No feedback provided", "Delayed feedback", "Immediate feedback with correct answers", "Feedback only on errors"], "answer": "Immediate feedback with correct answers", "difficulty": "easy" },
{ "id": 83, "question": "What is the purpose of the chain rule in backpropagation?", "options": ["To initialize weights", "To compute derivatives through composition", "To normalize gradients", "To prevent overfitting"], "answer": "To compute derivatives through composition", "difficulty": "easy" },
{ "id": 84, "question": "In multi-class classification, how many output neurons are needed for 5 classes?", "options": ["1", "4", "5", "10"], "answer": "5", "difficulty": "easy" },
{ "id": 85, "question": "What is the main advantage of ReLU over sigmoid activation?", "options": ["Bounded output", "Faster computation and less vanishing gradients", "Always positive output", "Smooth curve"], "answer": "Faster computation and less vanishing gradients", "difficulty": "easy" },
{ "id": 86, "question": "Which of the following is a characteristic of good training data?", "options": ["Small size", "Representative of target population", "All identical examples", "Only positive examples"], "answer": "Representative of target population", "difficulty": "easy" },
{ "id": 87, "question": "What happens during the backward pass in neural networks?", "options": ["Data flows forward", "Predictions are made", "Gradients are computed and weights updated", "Input is processed"], "answer": "Gradients are computed and weights updated", "difficulty": "easy" },
{ "id": 88, "question": "Which metric is best for imbalanced classification datasets?", "options": ["Accuracy", "F1-score", "Mean Squared Error", "R-squared"], "answer": "F1-score", "difficulty": "easy" },
{ "id": 89, "question": "What is transfer learning in neural networks?", "options": ["Moving data between computers", "Using pre-trained models for new tasks", "Converting between different architectures", "Transferring weights randomly"], "answer": "Using pre-trained models for new tasks", "difficulty": "easy" },
{ "id": 90, "question": "Which of the following can cause overfitting?", "options": ["Too little training data", "Model too complex for the data", "Training for too long", "All of the above"], "answer": "All of the above", "difficulty": "easy" },
{ "id": 91, "question": "What is the purpose of feature scaling in neural networks?", "options": ["To reduce model complexity", "To ensure features have similar ranges", "To increase accuracy", "To prevent underfitting"], "answer": "To ensure features have similar ranges", "difficulty": "easy" },
{ "id": 92, "question": "In neural networks, what is an epoch?", "options": ["Single forward pass", "Single backward pass", "Complete pass through all training data", "Model evaluation"], "answer": "Complete pass through all training data", "difficulty": "easy" },
{ "id": 93, "question": "Which of the following describes the bias-variance tradeoff?", "options": ["More bias always better", "Balance between model simplicity and flexibility", "Variance should always be zero", "Bias and variance are unrelated"], "answer": "Balance between model simplicity and flexibility", "difficulty": "easy" },
{ "id": 94, "question": "What is the main purpose of activation functions in neural networks?", "options": ["To speed up training", "To introduce non-linearity", "To reduce memory usage", "To prevent errors"], "answer": "To introduce non-linearity", "difficulty": "easy" },
{ "id": 95, "question": "Which of the following is true about neural network initialization?", "options": ["Weights should be initialized to zero", "Random initialization prevents symmetry", "All weights should be the same", "Initialization doesn't matter"], "answer": "Random initialization prevents symmetry", "difficulty": "easy" },
{ "id": 96, "question": "What is the role of the loss function during training?", "options": ["To preprocess data", "To guide weight updates by measuring error", "To select features", "To evaluate final performance"], "answer": "To guide weight updates by measuring error", "difficulty": "easy" },
{ "id": 97, "question": "In classification, what does 'ground truth' refer to?", "options": ["Model predictions", "Training algorithm", "Actual correct labels", "Input features"], "answer": "Actual correct labels", "difficulty": "easy" },
{ "id": 98, "question": "Which of the following best describes a neural network?", "options": ["Linear model", "Tree-based model", "Network of interconnected processing units", "Rule-based system"], "answer": "Network of interconnected processing units", "difficulty": "easy" },
{ "id": 99, "question": "What is the purpose of regularization in neural networks?", "options": ["To increase model complexity", "To prevent overfitting", "To speed up training", "To improve data quality"], "answer": "To prevent overfitting", "difficulty": "easy" },
{ "id": 100, "question": "In supervised learning, what is the relationship between input and output during training?", "options": ["Input and output are independent", "Output is used to learn mapping from input", "Input is derived from output", "There is no relationship"], "answer": "Output is used to learn mapping from input", "difficulty": "easy" },
{ "id": 101, "question": "A neural network for image classification has 784 input features (28x28 pixels) and needs to classify into 10 classes. If the hidden layer has 128 neurons, how many parameters are in the connection between input and hidden layer (including biases)?", "options": ["100,352", "100,480", "784", "128"], "answer": "100,480", "difficulty": "medium" },
{ "id": 102, "question": "Given a confusion matrix where TP=85, FP=15, TN=90, FN=10, what is the precision of the model?", "options": ["0.85", "0.895", "0.90", "0.85"], "answer": "0.85", "difficulty": "medium" },
{ "id": 103, "question": "A binary classifier outputs probabilities [0.8, 0.3, 0.6, 0.2, 0.9] for 5 samples with true labels [1, 0, 1, 0, 1]. Using threshold 0.5, what is the accuracy?", "options": ["0.6", "0.8", "1.0", "0.4"], "answer": "1.0", "difficulty": "medium" },
{ "id": 104, "question": "In a neural network with learning rate 0.01, if the gradient for a weight is -0.5, what will be the weight update?", "options": ["-0.005", "0.005", "-0.5", "0.01"], "answer": "0.005", "difficulty": "medium" },
{ "id": 105, "question": "A dataset has 1000 samples: 100 positive and 900 negative. If you use stratified sampling to create a 80-20 train-test split, how many positive samples will be in the training set?", "options": ["80", "100", "720", "200"], "answer": "80", "difficulty": "medium" },
{ "id": 106, "question": "For a multi-class classification problem with classes A, B, C, if the softmax outputs are [0.1, 0.7, 0.2], which class will be predicted and what is the confidence?", "options": ["Class A, 0.1", "Class B, 0.7", "Class C, 0.2", "Class B, 0.5"], "answer": "Class B, 0.7", "difficulty": "medium" },
{ "id": 107, "question": "A neural network has validation accuracy of 95% and training accuracy of 98%. What does this suggest about the model?", "options": ["Severe overfitting", "Underfitting", "Good generalization with slight overfitting", "Perfect model"], "answer": "Good generalization with slight overfitting", "difficulty": "medium" },
{ "id": 108, "question": "If you increase the learning rate in gradient descent, what is the most likely immediate effect?", "options": ["Slower convergence", "Faster convergence but risk of overshooting", "No effect on convergence", "Always better accuracy"], "answer": "Faster convergence but risk of overshooting", "difficulty": "medium" },
{ "id": 109, "question": "A ReLU activation function receives input -2. After applying ReLU and then computing the gradient during backpropagation, what is the gradient?", "options": ["0", "1", "-2", "2"], "answer": "0", "difficulty": "medium" },
{ "id": 110, "question": "In a batch of 32 samples, if 5 samples have loss values [0.1, 0.3, 0.2, 0.4, 0.1] and the rest have loss 0, what is the average batch loss?", "options": ["0.22", "0.034", "1.1", "0.1"], "answer": "0.034", "difficulty": "medium" },
{ "id": 111, "question": "For early stopping, you monitor validation loss for 10 epochs. The losses are [0.5, 0.4, 0.3, 0.35, 0.4, 0.45, 0.5, 0.48, 0.52, 0.51]. With patience=3, at which epoch should training stop?", "options": ["Epoch 6", "Epoch 7", "Epoch 8", "Continue training"], "answer": "Epoch 7", "difficulty": "medium" },
{ "id": 112, "question": "A neural network uses dropout with rate 0.5 during training. If a layer has 100 neurons, how many neurons are expected to be active during one forward pass?", "options": ["100", "50", "0", "75"], "answer": "50", "difficulty": "medium" },
{ "id": 113, "question": "Given weight decay (L2 regularization) with λ=0.01 and current weight w=0.8, if the gradient is -0.2, what is the total gradient including regularization?", "options": ["-0.192", "-0.208", "-0.2", "-0.18"], "answer": "-0.208", "difficulty": "medium" },
{ "id": 114, "question": "A classification model achieves 90% accuracy on a test set of 1000 samples. If the dataset is balanced with 2 classes, approximately how many samples were classified correctly?", "options": ["900", "450", "100", "500"], "answer": "900", "difficulty": "medium" },
{ "id": 115, "question": "In k-fold cross-validation with k=5, if your dataset has 1000 samples, how many samples are used for training in each fold?", "options": ["200", "800", "1000", "500"], "answer": "800", "difficulty": "medium" },
{ "id": 116, "question": "A sigmoid function outputs 0.8. What was the input to the sigmoid function approximately?", "options": ["0.8", "1.39", "0.2", "2.0"], "answer": "1.39", "difficulty": "medium" },
{ "id": 117, "question": "If you have a neural network with 3 hidden layers of sizes [64, 32, 16] and input size 100, output size 10, what is the total number of weight parameters (excluding biases)?", "options": ["6400", "9042", "9072", "8512"], "answer": "9072", "difficulty": "medium" },
{ "id": 118, "question": "A model's training loss decreases steadily but validation loss starts increasing after epoch 15. What should you do?", "options": ["Continue training", "Stop at epoch 15", "Increase learning rate", "Add more layers"], "answer": "Stop at epoch 15", "difficulty": "medium" },
{ "id": 119, "question": "For a binary classification problem, if sensitivity (recall) is 0.8 and specificity is 0.9, what is the balanced accuracy?", "options": ["0.8", "0.9", "0.85", "0.72"], "answer": "0.85", "difficulty": "medium" },
{ "id": 120, "question": "A batch normalization layer normalizes inputs to have mean 0 and standard deviation 1. If the original batch has mean=5 and std=2, what happens to a data point with value 9?", "options": ["Becomes 2", "Becomes 9", "Becomes 1", "Becomes 4"], "answer": "Becomes 2", "difficulty": "medium" },
{ "id": 121, "question": "In gradient descent, if the cost function is J(w) = w² + 2w + 1, what is the gradient when w = 3?", "options": ["8", "6", "16", "2"], "answer": "8", "difficulty": "medium" },
{ "id": 122, "question": "A neural network uses He initialization for a layer with 64 input connections. What is the standard deviation used for weight initialization?", "options": ["√(1/64)", "√(2/64)", "√(6/64)", "1/64"], "answer": "√(2/64)", "difficulty": "medium" },
{ "id": 123, "question": "If you apply data augmentation that increases your training set size by 4x, and your original training time was 2 hours, what is the expected new training time per epoch?", "options": ["2 hours", "8 hours", "0.5 hours", "4 hours"], "answer": "8 hours", "difficulty": "medium" },
{ "id": 124, "question": "A confusion matrix shows: Predicted Class A: 50 correct, 10 incorrect; Predicted Class B: 5 incorrect, 35 correct. What is the overall accuracy?", "options": ["0.85", "0.75", "0.90", "0.80"], "answer": "0.85", "difficulty": "medium" },
{ "id": 125, "question": "In a neural network, if you use momentum with β=0.9, and previous velocity was 0.1, current gradient is 0.5, what is the new velocity?", "options": ["0.59", "0.41", "0.5", "0.6"], "answer": "0.59", "difficulty": "medium" },
{ "id": 126, "question": "A model trained on RGB images (3 channels) needs to be adapted for grayscale images (1 channel). What modification is needed in the first layer?", "options": ["Change output size", "Change input size from 3 to 1", "Add more layers", "Change activation function"], "answer": "Change input size from 3 to 1", "difficulty": "medium" },
{ "id": 127, "question": "Given learning curves where training error decreases but validation error plateaus at a high value, what is the primary issue?", "options": ["Overfitting", "Underfitting", "Perfect fit", "Data leakage"], "answer": "Underfitting", "difficulty": "medium" },
{ "id": 128, "question": "If you implement gradient clipping with maximum norm 1.0, and the gradient norm is 2.5, what will be the scaling factor applied?", "options": ["0.4", "0.5", "1.0", "2.5"], "answer": "0.4", "difficulty": "medium" },
{ "id": 129, "question": "A neural network processes inputs in batches of 64. If your dataset has 1000 samples, how many complete batches are there in one epoch?", "options": ["15", "16", "15.625", "64"], "answer": "15", "difficulty": "medium" },
{ "id": 130, "question": "For a multi-class problem with 5 classes using one-hot encoding, what is the shape of the target vector for one sample?", "options": ["(1,)", "(5,)", "(1, 5)", "(5, 1)"], "answer": "(5,)", "difficulty": "medium" },
{ "id": 131, "question": "If you double the batch size in SGD training, what typically happens to the training dynamics?", "options": ["Faster convergence, more noise", "Slower convergence, less noise", "No change", "Always better accuracy"], "answer": "Slower convergence, less noise", "difficulty": "medium" },
{ "id": 132, "question": "A model achieves F1-score of 0.8 with precision 0.75. What is the recall?", "options": ["0.857", "0.8", "0.75", "0.6"], "answer": "0.857", "difficulty": "medium" },
{ "id": 133, "question": "In transfer learning, you freeze the first 3 layers of a pre-trained model. During training, which parameters get updated?", "options": ["All parameters", "Only last layer parameters", "Parameters from layer 4 onwards", "Only the first 3 layers"], "answer": "Parameters from layer 4 onwards", "difficulty": "medium" },
{ "id": 134, "question": "A ReLU layer has 100 neurons, and during forward pass, 30 neurons output zero. During backpropagation, how many neurons will have non-zero gradients?", "options": ["100", "70", "30", "0"], "answer": "70", "difficulty": "medium" },
{ "id": 135, "question": "If you apply L1 regularization with λ=0.01 to a weight with value 0.5, what is the regularization term's contribution to the gradient?", "options": ["0.005", "0.01", "0.5", "0.25"], "answer": "0.01", "difficulty": "medium" },
{ "id": 136, "question": "A neural network uses Xavier initialization for a fully connected layer with 50 inputs and 100 outputs. What is the variance of the weight distribution?", "options": ["1/50", "2/150", "1/75", "2/50"], "answer": "2/150", "difficulty": "medium" },
{ "id": 137, "question": "In a classification task with severe class imbalance (1:99 ratio), a model predicting always the majority class achieves 99% accuracy. What's wrong with using accuracy as a metric?", "options": ["Nothing wrong", "Doesn't reflect minority class performance", "Too high value", "Not normalized"], "answer": "Doesn't reflect minority class performance", "difficulty": "medium" },
{ "id": 138, "question": "During backpropagation in a 3-layer network, if the gradient at the output is 0.1 and the weights between layers are [0.5, 0.8], what is the gradient magnitude at the input layer approximately?", "options": ["0.04", "0.1", "0.13", "0.9"], "answer": "0.04", "difficulty": "medium" },
{ "id": 139, "question": "A model uses leaky ReLU with α=0.01. If the input is -2, what is the output and what is the gradient during backpropagation?", "options": ["Output: -0.02, Gradient: 0.01", "Output: 0, Gradient: 0", "Output: -2, Gradient: 1", "Output: -0.02, Gradient: 1"], "answer": "Output: -0.02, Gradient: 0.01", "difficulty": "medium" },
{ "id": 140, "question": "If you train a model for 50 epochs with learning rate decay of 0.1 every 10 epochs, starting with lr=0.1, what is the learning rate at epoch 25?", "options": ["0.001", "0.01", "0.1", "0.0001"], "answer": "0.001", "difficulty": "medium" },
{ "id": 141, "question": "A convolutional layer processes 28x28 images with 5x5 filters, stride=1, padding=0. What is the output spatial dimension?", "options": ["24x24", "28x28", "23x23", "32x32"], "answer": "24x24", "difficulty": "medium" },
{ "id": 142, "question": "In Adam optimizer, if β₁=0.9, β₂=0.999, current gradient=0.1, previous first moment=0.05, what is the new first moment?", "options": ["0.055", "0.1", "0.045", "0.095"], "answer": "0.055", "difficulty": "medium" },
{ "id": 143, "question": "A neural network has learning curves where both training and validation loss decrease together but remain high. What does this indicate?", "options": ["Overfitting", "Good fit", "Underfitting", "Data leakage"], "answer": "Underfitting", "difficulty": "medium" },
{ "id": 144, "question": "If you use temperature scaling T=2 with softmax on logits [4, 2, 1], what are the approximate probabilities?", "options": ["[0.67, 0.24, 0.09]", "[0.84, 0.12, 0.04]", "[0.5, 0.33, 0.17]", "[0.57, 0.29, 0.14]"], "answer": "[0.57, 0.29, 0.14]", "difficulty": "medium" },
{ "id": 145, "question": "A model trained with batch size 32 takes 1 hour for 100 epochs. If you change batch size to 16, approximately how long will 100 epochs take?", "options": ["0.5 hours", "1 hour", "2 hours", "4 hours"], "answer": "2 hours", "difficulty": "medium" },
{ "id": 146, "question": "In a neural network with skip connections, if the input to a residual block is x and the block's function is F(x), what is the output?", "options": ["F(x)", "x + F(x)", "x - F(x)", "x * F(x)"], "answer": "x + F(x)", "difficulty": "medium" },
{ "id": 147, "question": "A classifier outputs class probabilities [0.6, 0.3, 0.1] for a sample with true label class 2 (index 1). What is the cross-entropy loss?", "options": ["1.2", "0.51", "0.3", "1.61"], "answer": "1.2", "difficulty": "medium" },
{ "id": 148, "question": "If you apply dropout with probability 0.3 during training, what scaling is applied to the remaining neurons?", "options": ["0.3", "0.7", "1.43", "1.0"], "answer": "1.43", "difficulty": "medium" },
{ "id": 149, "question": "A dataset has features with ranges: Feature A [0-1000], Feature B [0-1]. After min-max normalization, what will be the range of Feature A?", "options": ["[0-1000]", "[0-1]", "[-1-1]", "[0-100]"], "answer": "[0-1]", "difficulty": "medium" },
{ "id": 150, "question": "In ensemble learning, if 5 models predict probabilities [0.8, 0.6, 0.7, 0.9, 0.5] for class 1, what is the ensemble prediction using averaging?", "options": ["0.8", "0.7", "0.9", "0.6"], "answer": "0.7", "difficulty": "medium" },
{ "id": 151, "question": "A model uses cyclical learning rate with base_lr=0.001, max_lr=0.01, step_size=100. What is the learning rate at step 50 in the first cycle?", "options": ["0.001", "0.0055", "0.01", "0.005"], "answer": "0.0055", "difficulty": "medium" },
{ "id": 152, "question": "If gradient norm is 5.0 and you apply gradient clipping with max_norm=2.0, what is the clipped gradient norm?", "options": ["5.0", "2.0", "0.4", "3.0"], "answer": "2.0", "difficulty": "medium" },
{ "id": 153, "question": "A neural network layer has weight matrix W of shape (100, 50). After applying L2 weight decay with λ=0.01, what is added to the loss function?", "options": ["0.01 * ||W||₁", "0.01 * ||W||₂²", "0.01 * trace(W)", "0.01 * det(W)"], "answer": "0.01 * ||W||₂²", "difficulty": "medium" },
{ "id": 154, "question": "In focal loss with α=0.25, γ=2, for a sample with p=0.9 (high confidence correct prediction), how does the loss compare to standard cross-entropy?", "options": ["Much higher", "Slightly higher", "Much lower", "Same"], "answer": "Much lower", "difficulty": "medium" },
{ "id": 155, "question": "A model processes sequences of length 10 with embedding dimension 128. If batch size is 32, what is the shape of the embedded input?", "options": ["(32, 10, 128)", "(32, 128, 10)", "(10, 32, 128)", "(128, 32, 10)"], "answer": "(32, 10, 128)", "difficulty": "medium" },
{ "id": 156, "question": "If you use label smoothing with ε=0.1 for a 3-class problem, what are the smoothed labels for a sample with true class 1 (index 0)?", "options": ["[0.9, 0.05, 0.05]", "[1, 0, 0]", "[0.33, 0.33, 0.33]", "[0.8, 0.1, 0.1]"], "answer": "[0.9, 0.05, 0.05]", "difficulty": "medium" },
{ "id": 157, "question": "A neural network has 3 classes with logits [2, 1, 3]. After applying softmax, which class has the highest probability and what is its approximate value?", "options": ["Class 0, 0.24", "Class 1, 0.09", "Class 2, 0.67", "Class 2, 0.90"], "answer": "Class 2, 0.67", "difficulty": "medium" },
{ "id": 158, "question": "In stratified k-fold cross-validation with k=4 and a dataset having 80% class A and 20% class B (1000 samples total), how many class B samples are in each fold?", "options": ["50", "200", "20", "25"], "answer": "50", "difficulty": "medium" },
{ "id": 159, "question": "A model uses cosine annealing for learning rate scheduling. If initial lr=0.1, minimum lr=0.001, and T_max=100, what is the lr at step 50?", "options": ["0.001", "0.0505", "0.1", "0.05"], "answer": "0.001", "difficulty": "medium" },
{ "id": 160, "question": "If you apply mixup data augmentation with λ=0.4 to two samples with labels [1,0,0] and [0,1,0], what is the mixed label?", "options": ["[1,0,0]", "[0.4,0.6,0]", "[0.6,0.4,0]", "[0,1,0]"], "answer": "[0.6,0.4,0]", "difficulty": "medium" },
{ "id": 161, "question": "A model shows training accuracy increasing to 99% while validation accuracy stays at 75%. The training loss is 0.01 and validation loss is 1.2. What is the primary issue?", "options": ["Underfitting", "Perfect fit", "Severe overfitting", "Data leakage"], "answer": "Severe overfitting", "difficulty": "medium" },
{ "id": 162, "question": "In a neural network using batch normalization, during inference with a batch size of 1, which statistics are used for normalization?", "options": ["Current batch statistics", "Running average statistics from training", "All zeros", "Random statistics"], "answer": "Running average statistics from training", "difficulty": "medium" },
{ "id": 163, "question": "If you use weighted cross-entropy loss with class weights [1, 3, 2] for classes [A, B, C], how does this affect the learning?", "options": ["Equal emphasis on all classes", "More emphasis on class B", "Less emphasis on class A", "More emphasis on classes B and C"], "answer": "More emphasis on classes B and C", "difficulty": "medium" },
{ "id": 164, "question": "A model uses learning rate warm-up for 1000 steps, starting from 0 to 0.001. What is the learning rate at step 500?", "options": ["0", "0.0005", "0.001", "0.0001"], "answer": "0.0005", "difficulty": "medium" },
{ "id": 165, "question": "In a neural network with layers [784, 256, 128, 10], if you apply dropout with rate 0.5 to all hidden layers during training, what fraction of the total network weights are actually being used in one forward pass?", "options": ["0.5", "0.25", "Varies per layer", "0.75"], "answer": "Varies per layer", "difficulty": "medium" },
{ "id": 166, "question": "A classification model has macro-averaged F1 of 0.8 and weighted-averaged F1 of 0.85. What can you infer about the dataset?", "options": ["Balanced classes", "Some class imbalance with better performance on frequent classes", "Perfect balance", "Cannot determine"], "answer": "Some class imbalance with better performance on frequent classes", "difficulty": "medium" },
{ "id": 167, "question": "If you use polynomial learning rate decay with power=0.9, initial_lr=0.1, and 1000 total steps, what is the learning rate at step 900?", "options": ["0.01", "0.013", "0.1", "0.009"], "answer": "0.013", "difficulty": "medium" },
{ "id": 168, "question": "A model trained on 32x32 RGB images needs to handle 64x64 RGB images. Which layer needs modification?", "options": ["All layers", "Only the first layer", "Only the last layer", "Only hidden layers"], "answer": "Only the first layer", "difficulty": "medium" },
{ "id": 169, "question": "In a multi-task learning setup, if Task A has loss 0.5 and Task B has loss 2.0, and you use equal weighting, what is the total loss?", "options": ["1.25", "2.5", "0.5", "2.0"], "answer": "1.25", "difficulty": "medium" },
{ "id": 170, "question": "A neural network uses swish activation f(x) = x * sigmoid(x). If the input is 2, what is the approximate output?", "options": ["1.76", "2.0", "0.88", "1.0"], "answer": "1.76", "difficulty": "medium" },
{ "id": 171, "question": "If you implement custom gradient accumulation over 4 mini-batches before updating weights, and each mini-batch has loss [0.2, 0.3, 0.1, 0.4], what is the effective batch loss?", "options": ["0.4", "0.25", "1.0", "0.2"], "answer": "0.25", "difficulty": "medium" },
{ "id": 172, "question": "A model uses spectral normalization on a weight matrix W. If the largest singular value of W is 3.5, what will be the largest singular value after normalization?", "options": ["3.5", "1.0", "0", "Depends on other singular values"], "answer": "1.0", "difficulty": "medium" },
{ "id": 173, "question": "In self-supervised pretraining followed by supervised fine-tuning, you freeze the encoder and only train the classifier head. If the encoder has 1M parameters and classifier has 10K parameters, how many parameters are updated during fine-tuning?", "options": ["1M", "10K", "1.01M", "0"], "answer": "10K", "difficulty": "medium" },
{ "id": 174, "question": "A model uses truncated normal initialization with mean=0, std=0.1, and truncation at 2 standard deviations. What is the range of initialized weights?", "options": ["[-0.1, 0.1]", "[-0.2, 0.2]", "[-2, 2]", "[0, 0.2]"], "answer": "[-0.2, 0.2]", "difficulty": "medium" },
{ "id": 175, "question": "If you apply cutmix data augmentation with λ=0.3, what fraction of the image is replaced and what are the mixed labels for samples with original labels [1,0] and [0,1]?", "options": ["30% replaced, labels [0.7,0.3]", "70% replaced, labels [0.3,0.7]", "30% replaced, labels [0.3,0.7]", "50% replaced, labels [0.5,0.5]"], "answer": "30% replaced, labels [0.7,0.3]", "difficulty": "medium" },
{ "id": 176, "question": "A neural network uses group normalization with 8 groups on a feature map of shape (batch=32, channels=16, height=28, width=28). How many parameters does the group normalization layer have?", "options": ["16", "32", "8", "64"], "answer": "32", "difficulty": "medium" },
{ "id": 177, "question": "In knowledge distillation, if the teacher model outputs [0.7, 0.2, 0.1] and student outputs [0.5, 0.3, 0.2] for the same input, which component of the loss focuses on matching these distributions?", "options": ["Hard label loss", "Soft label loss", "Feature matching loss", "Attention loss"], "answer": "Soft label loss", "difficulty": "medium" },
{ "id": 178, "question": "A model uses scheduled sampling during training sequences. If the sampling probability starts at 0 (always use ground truth) and increases to 0.5 over 1000 steps, what is the probability at step 750?", "options": ["0.375", "0.5", "0.25", "0.75"], "answer": "0.375", "difficulty": "medium" },
{ "id": 179, "question": "If you apply lookahead optimizer with k=5 and α=0.5 on top of SGD, how often are the slow weights updated?", "options": ["Every step", "Every 5 steps", "Every 10 steps", "Once per epoch"], "answer": "Every 5 steps", "difficulty": "medium" },
{ "id": 180, "question": "A model processes variable-length sequences using masking. If a batch has sequences of lengths [10, 7, 15, 12] padded to length 15, what fraction of the computations are on actual data vs padding?", "options": ["0.73", "0.6", "0.8", "0.67"], "answer": "0.73", "difficulty": "medium" },
{ "id": 181, "question": "You're designing a neural architecture for a severely imbalanced dataset (99.9% negative, 0.1% positive) with critical false negative constraints. Evaluate the best combination of techniques: Which approach would be most effective?", "options": ["Focal loss + class weighting + precision-recall curve optimization", "Standard cross-entropy + oversampling + F1 optimization", "Weighted focal loss + threshold tuning + cost-sensitive learning", "SMOTE + standard loss + accuracy optimization"], "answer": "Weighted focal loss + threshold tuning + cost-sensitive learning", "difficulty": "hard" },
{ "id": 182, "question": "Design a custom loss function for a multi-class classification problem where misclassifying class A as class B costs 10x more than other misclassifications. If standard cross-entropy loss is L_ce and you have cost matrix C, which formulation is most appropriate?", "options": ["L = L_ce * C[true_class, predicted_class]", "L = L_ce + λ * C[true_class, predicted_class]", "L = Σ(C[i,j] * p_j * log(p_j)) for true class i", "L = -Σ(C[true_class,j] * log(p_j)) for all j"], "answer": "L = -Σ(C[true_class,j] * log(p_j)) for all j", "difficulty": "hard" },
{ "id": 183, "question": "You observe that your neural network's gradients have norms following the sequence [2.1, 1.8, 1.5, 1.2, 0.9, 0.6, 0.3, 0.1] across layers from output to input. Design a layer-wise learning rate schedule to address this. Which approach is most theoretically sound?", "options": ["lr_layer = base_lr * sqrt(layer_depth)", "lr_layer = base_lr * gradient_norm_ratio", "lr_layer = base_lr / (1 + layer_index)", "lr_layer = base_lr * (gradient_norm / max_gradient_norm)"], "answer": "lr_layer = base_lr * (gradient_norm / max_gradient_norm)", "difficulty": "hard" },
{ "id": 184, "question": "Evaluate a novel neural architecture where skip connections follow the pattern: layer i connects to layers i+1, i+3, i+7, i+15 (powers of 2 minus 1). For a 32-layer network, analyze the gradient flow. What is the primary advantage of this design?", "options": ["Reduces computational complexity quadratically", "Creates logarithmic gradient pathways reducing vanishing gradients", "Enables better feature reuse at multiple scales", "Provides exponential increase in model capacity"], "answer": "Creates logarithmic gradient pathways reducing vanishing gradients", "difficulty": "hard" },
{ "id": 185, "question": "Design an adaptive regularization scheme where dropout probability adapts based on validation performance. If current val_acc = 0.85, train_acc = 0.95, and you want to target a gap of 0.02, which adaptive rule is most effective?", "options": ["dropout_rate = 0.5 * (train_acc - val_acc)", "dropout_rate = min(0.8, 0.1 + (train_acc - val_acc - 0.02) * 2)", "dropout_rate = sigmoid(10 * (train_acc - val_acc - 0.02))", "dropout_rate = max(0, (train_acc - val_acc) / train_acc)"], "answer": "dropout_rate = min(0.8, 0.1 + (train_acc - val_acc - 0.02) * 2)", "difficulty": "hard" },
{ "id": 186, "question": "You're implementing a meta-learning approach where a neural network learns to predict optimal learning rates for different layers based on gradient statistics. Design the meta-network input features. Which combination provides the most information?", "options": ["[gradient_norm, weight_norm, layer_depth]", "[gradient_mean, gradient_var, gradient_skew, weight_spectral_norm, loss_hessian_trace]", "[current_lr, loss_value, accuracy]", "[gradient_direction_change, weight_update_magnitude]"], "answer": "[gradient_mean, gradient_var, gradient_skew, weight_spectral_norm, loss_hessian_trace]", "difficulty": "hard" },
{ "id": 187, "question": "Evaluate the theoretical implications of using different activation functions in a 50-layer network. Consider gradient flow, expressivity, and optimization landscape. Which combination optimizes all three aspects?", "options": ["All ReLU for simplicity and speed", "Alternating ReLU and Tanh for balanced gradients", "GELU in early layers, Swish in middle, Linear in final", "Learnable activation functions with bounded derivatives"], "answer": "Learnable activation functions with bounded derivatives", "difficulty": "hard" },
{ "id": 188, "question": "Design a curriculum learning strategy for a classification task with hierarchical labels (e.g., animal -> mammal -> dog -> breed). The model should learn from coarse to fine-grained classifications. Which curriculum design is most effective?", "options": ["Train on coarse labels first, then fine-tune on fine labels", "Gradually increase label granularity with weighted multi-task loss", "Use progressive label smoothing from uniform to one-hot", "Employ hierarchical softmax with curriculum on tree depth"], "answer": "Gradually increase label granularity with weighted multi-task loss", "difficulty": "hard" },
{ "id": 189, "question": "Analyze a scenario where your model's Hessian eigenvalues range from 1e-6 to 1e6 (condition number = 1e12). Design an optimization strategy that addresses this ill-conditioning. Which approach is most theoretically grounded?", "options": ["Use very small learning rates (1e-8)", "Implement natural gradient descent with Fisher information", "Apply preconditioning with approximate Hessian inverse", "Use adaptive learning rates per parameter with second-order information"], "answer": "Apply preconditioning with approximate Hessian inverse", "difficulty": "hard" },
{ "id": 190, "question": "You're designing a neural architecture that needs to be invariant to input permutations while maintaining expressivity for classification. Evaluate the trade-offs between different approaches. Which design principle is most effective?", "options": ["Use pooling operations after each layer", "Employ permutation-invariant layers with set aggregation functions", "Sort inputs deterministically before processing", "Use attention mechanisms with symmetric computations"], "answer": "Employ permutation-invariant layers with set aggregation functions", "difficulty": "hard" },
{ "id": 191, "question": "Evaluate a multi-objective optimization scenario where you need to simultaneously minimize classification error and maximize model interpretability. Design a Pareto-optimal solution approach. Which formulation is most principled?", "options": ["Weighted sum: α*loss + β*interpretability_penalty", "Constraint optimization: minimize loss subject to interpretability ≥ threshold", "Multi-objective evolutionary algorithm with Pareto ranking", "Alternating optimization between accuracy and interpretability"], "answer": "Multi-objective evolutionary algorithm with Pareto ranking", "difficulty": "hard" },
{ "id": 192, "question": "Design a novel attention mechanism for classification that incorporates uncertainty quantification. The attention weights should reflect both relevance and confidence. Which mathematical formulation is most appropriate?", "options": ["α_i = softmax(W_q^T * h_i) * σ(W_u^T * h_i)", "α_i = softmax(W_q^T * h_i / sqrt(uncertainty_i))", "α_i = (W_q^T * h_i) * exp(-uncertainty_i) / Z", "α_i = Beta(relevance_i, uncertainty_i) normalized"], "answer": "α_i = (W_q^T * h_i) * exp(-uncertainty_i) / Z", "difficulty": "hard" },
{ "id": 193, "question": "Analyze the theoretical capacity of a neural network with L layers, each having n neurons, for a k-class classification problem. Consider both VC dimension and Rademacher complexity. Which bound is tightest?", "options": ["O(L*n*log(k))", "O(L*n^2*k)", "O(sqrt(L*n*log(L*n*k)))", "O(L*n*k*log(L*n))"], "answer": "O(sqrt(L*n*log(L*n*k)))", "difficulty": "hard" },
{ "id": 194, "question": "Design a dynamic neural architecture that can adaptively change its depth during training based on task complexity. Evaluate different adaptation criteria. Which approach is most theoretically sound?", "options": ["Add layers when training loss plateaus", "Use gradient variance to determine architecture changes", "Employ information-theoretic measures of task complexity", "Base decisions on validation performance trends"], "answer": "Employ information-theoretic measures of task complexity", "difficulty": "hard" },
{ "id": 195, "question": "You're implementing a neural network that needs to handle concept drift in streaming classification data. Design an adaptation mechanism that balances plasticity and stability. Which approach optimizes this trade-off?", "options": ["Exponential moving average of model parameters", "Elastic weight consolidation with dynamic importance weights", "Meta-learning with gradient-based adaptation", "Continual learning with experience replay and adaptive regularization"], "answer": "Continual learning with experience replay and adaptive regularization", "difficulty": "hard" },
{ "id": 196, "question": "Evaluate the theoretical implications of using different weight initialization schemes in very deep networks (>100 layers). Consider signal propagation, gradient flow, and training dynamics. Which initialization strategy is optimal?", "options": ["Scaled Xavier with layer-dependent variance", "Dynamic initialization based on activation statistics", "Orthogonal initialization with gain scaling", "LSUV (Layer-Sequential Unit-Variance) initialization"], "answer": "LSUV (Layer-Sequential Unit-Variance) initialization", "difficulty": "hard" },
{ "id": 197, "question": "Design a probabilistic neural network for classification that outputs both predictions and epistemic uncertainty estimates. Which architecture and training procedure is most principled?", "options": ["Bayesian neural network with variational inference", "Ensemble of networks with Monte Carlo sampling", "Single network with evidential learning", "Deep Gaussian processes with sparse inducing points"], "answer": "Single network with evidential learning", "difficulty": "hard" },
{ "id": 198, "question": "Analyze a scenario where you need to train a neural network with limited labels but abundant unlabeled data. Design a semi-supervised learning approach that maximizes information utilization. Which method is most effective?", "options": ["Co-training with multiple views", "Pseudo-labeling with confidence thresholding", "Virtual adversarial training with consistency regularization", "Mutual information maximization between labeled and unlabeled features"], "answer": "Virtual adversarial training with consistency regularization", "difficulty": "hard" },
{ "id": 199, "question": "Evaluate the design of a neural architecture for few-shot classification. The model should generalize to new classes with minimal examples. Which architectural principle is most crucial?", "options": ["Parameter sharing across classes", "Metric learning with learnable distance functions", "Attention-based prototype networks with episodic training", "Meta-learning with gradient-based optimization"], "answer": "Attention-based prototype networks with episodic training", "difficulty": "hard" },
{ "id": 200, "question": "Design a neural network training procedure that is robust to adversarial examples while maintaining clean accuracy. Evaluate different defense strategies. Which approach provides the best robustness-accuracy trade-off?", "options": ["Adversarial training with PGD attacks", "Certified defense with randomized smoothing", "Feature denoising with auxiliary networks", "Adversarial training with curriculum learning and multiple threat models"], "answer": "Adversarial training with curriculum learning and multiple threat models", "difficulty": "hard" },
{ "id": 201, "question": "Analyze the convergence properties of a neural network trained with a novel optimizer that combines momentum, adaptive learning rates, and second-order information. Which convergence guarantee is most realistic?", "options": ["Global convergence to global minimum", "Convergence to stationary points with probability 1", "Local convergence with linear rate near critical points", "Convergence to approximate stationary points with high probability"], "answer": "Convergence to approximate stationary points with high probability", "difficulty": "hard" },
{ "id": 202, "question": "Design a neural architecture that can perform multi-scale feature learning for classification tasks with objects at different scales. Evaluate architectural choices for optimal scale invariance. Which design is most effective?", "options": ["Pyramid pooling with multi-scale feature fusion", "Dilated convolutions with exponential dilation rates", "Feature pyramid networks with lateral connections", "Scale-space convolutions with learnable scale parameters"], "answer": "Scale-space convolutions with learnable scale parameters", "difficulty": "hard" },
{ "id": 203, "question": "Evaluate a neural network training scenario where the loss surface has multiple local minima with significantly different generalization properties. Design an exploration strategy to find better minima. Which approach is most promising?", "options": ["Simulated annealing with adaptive temperature", "Stochastic weight averaging with cyclic learning rates", "Mode connectivity training with path optimization", "Evolutionary strategies with population-based search"], "answer": "Mode connectivity training with path optimization", "difficulty": "hard" },
{ "id": 204, "question": "Design a neural network that can adaptively allocate computational resources based on input complexity. Different inputs should trigger different computational paths. Which gating mechanism is most efficient?", "options": ["Soft attention gates with differentiable routing", "Hard attention with reinforcement learning", "Input-dependent early exit with confidence thresholds", "Mixture of experts with learned routing"], "answer": "Input-dependent early exit with confidence thresholds", "difficulty": "hard" },
{ "id": 205, "question": "Analyze the theoretical relationship between neural network width, depth, and approximation capability for classification boundaries. Which scaling law is most accurate for practical networks?", "options": ["Approximation error decreases as O(1/width)", "Error decreases exponentially with depth", "Error decreases as O(1/sqrt(parameters))", "Error follows power law with network complexity"], "answer": "Error decreases as O(1/sqrt(parameters))", "difficulty": "hard" },
{ "id": 206, "question": "Design a training procedure for a neural network that must maintain performance across multiple domains simultaneously (multi-domain learning). Evaluate catastrophic forgetting mitigation strategies. Which approach is most effective?", "options": ["Domain-specific batch normalization", "Progressive neural networks with lateral connections", "Memory-augmented networks with episodic recall", "Gradient projection methods with domain-specific constraints"], "answer": "Memory-augmented networks with episodic recall", "difficulty": "hard" },
{ "id": 207, "question": "Evaluate the design of a neural architecture for hierarchical multi-label classification where labels have tree structure. The model should respect label dependencies. Which architectural constraint is most important?", "options": ["Soft label hierarchy consistency loss", "Hard hierarchical constraints in output layer", "Hierarchical attention with parent-child dependencies", "Multi-task learning with shared hierarchical representations"], "answer": "Hierarchical attention with parent-child dependencies", "difficulty": "hard" },
{ "id": 208, "question": "Design a neural network training algorithm that can automatically discover the optimal architecture during training (neural architecture search). Which search strategy balances efficiency and effectiveness?", "options": ["Reinforcement learning with controller network", "Differentiable architecture search with continuous relaxation", "Evolutionary algorithms with mutation operators", "Bayesian optimization with Gaussian process surrogate"], "answer": "Differentiable architecture search with continuous relaxation", "difficulty": "hard" },
{ "id": 209, "question": "Analyze a scenario where you need to compress a large neural network for deployment while maintaining classification accuracy. Evaluate different compression techniques. Which combination is most effective?", "options": ["Pruning + quantization + knowledge distillation", "Low-rank factorization + weight sharing", "Neural architecture search for efficient architectures", "Progressive shrinking with performance-aware optimization"], "answer": "Pruning + quantization + knowledge distillation", "difficulty": "hard" },
{ "id": 210, "question": "Design a neural network that can provide explanations for its classification decisions. The explanations should be both faithful and interpretable. Which approach best balances these requirements?", "options": ["Gradient-based saliency maps", "Attention visualization", "Concept activation vectors with TCAV", "Counterfactual explanation generation"], "answer": "Concept activation vectors with TCAV", "difficulty": "hard" },
{ "id": 211, "question": "Evaluate the theoretical guarantees of different neural network training algorithms under noisy label conditions. Which approach provides the strongest robustness guarantees?", "options": ["Robust loss functions with outlier resistance", "Meta-learning for noise adaptation", "Co-teaching with disagreement-based sample selection", "Certified training with worst-case noise bounds"], "answer": "Certified training with worst-case noise bounds", "difficulty": "hard" },
{ "id": 212, "question": "Design a neural architecture that can perform online learning with streaming data while avoiding catastrophic forgetting. The model should adapt quickly to new patterns. Which memory mechanism is most effective?", "options": ["Episodic memory with neural Turing machines", "Synaptic consolidation with importance weighting", "Complementary learning systems with replay", "Meta-learning with fast adaptation capabilities"], "answer": "Complementary learning systems with replay", "difficulty": "hard" },
{ "id": 213, "question": "Analyze the optimization landscape of a neural network with batch normalization. Consider the effects on loss surface geometry and training dynamics. Which property is most significant?", "options": ["Smoother loss landscape with reduced curvature", "Improved gradient flow through normalization", "Reduced internal covariate shift", "Implicit regularization through noise injection"], "answer": "Smoother loss landscape with reduced curvature", "difficulty": "hard" },
{ "id": 214, "question": "Design a neural network training procedure that is provably fair across different demographic groups while maintaining high accuracy. Which fairness constraint is most mathematically principled?", "options": ["Demographic parity with equal positive rates", "Equalized odds with equal TPR and FPR", "Individual fairness with Lipschitz continuity", "Counterfactual fairness with causal modeling"], "answer": "Individual fairness with Lipschitz continuity", "difficulty": "hard" },
{ "id": 215, "question": "Evaluate the design of a neural architecture that can handle variable input dimensions dynamically (e.g., images of different resolutions). Which approach provides the most flexibility?", "options": ["Adaptive pooling layers", "Multi-scale feature extractors", "Set-based neural networks", "Continuous convolutions with coordinate networks"], "answer": "Continuous convolutions with coordinate networks", "difficulty": "hard" },
{ "id": 216, "question": "Design a training algorithm for neural networks that can automatically balance exploration and exploitation during optimization. Which approach is most theoretically grounded?", "options": ["Thompson sampling for parameter updates", "Upper confidence bounds for learning rate adaptation", "Information-theoretic exploration with mutual information", "Bayesian optimization with acquisition functions"], "answer": "Information-theoretic exploration with mutual information", "difficulty": "hard" },
{ "id": 217, "question": "Analyze the theoretical properties of different neural network regularization techniques in the overparameterized regime. Which technique provides the best generalization bounds?", "options": ["L2 regularization with optimal weight decay", "Dropout with adaptive rates", "Early stopping with validation monitoring", "Spectral normalization with Lipschitz constraints"], "answer": "Spectral normalization with Lipschitz constraints", "difficulty": "hard" },
{ "id": 218, "question": "Design a neural architecture for active learning in classification tasks. The model should identify the most informative samples for labeling. Which uncertainty quantification method is most effective?", "options": ["Bayesian neural networks with epistemic uncertainty", "Deep ensembles with prediction variance", "Monte Carlo dropout with stochastic forward passes", "Evidential deep learning with uncertainty measures"], "answer": "Evidential deep learning with uncertainty measures", "difficulty": "hard" },
{ "id": 219, "question": "Evaluate the convergence properties of federated learning with neural networks across heterogeneous clients. Design an aggregation strategy that handles statistical heterogeneity. Which approach is most robust?", "options": ["FedAvg with momentum", "Personalized federated learning", "Robust aggregation with Byzantine tolerance", "Adaptive federated optimization with client-specific learning rates"], "answer": "Adaptive federated optimization with client-specific learning rates", "difficulty": "hard" },
{ "id": 220, "question": "Design a neural network that can perform zero-shot classification by learning compositional representations. The model should generalize to unseen class combinations. Which learning objective is most appropriate?", "options": ["Contrastive learning with semantic embeddings", "Variational autoencoders with disentangled representations", "Compositional attention with part-whole relationships", "Graph neural networks with semantic knowledge graphs"], "answer": "Graph neural networks with semantic knowledge graphs", "difficulty": "hard" },
{ "id": 221, "question": "Analyze the sample complexity of learning with neural networks under different data distributions. Consider both labeled and unlabeled data availability. Which bound is tightest for practical scenarios?", "options": ["PAC-Bayes bounds with data-dependent priors", "Rademacher complexity with empirical process theory", "Information-theoretic bounds with mutual information", "Stability-based generalization bounds"], "answer": "Stability-based generalization bounds", "difficulty": "hard" },
{ "id": 222, "question": "Design a neural architecture that can learn causal relationships for classification while being robust to spurious correlations. Which causal learning approach is most effective?", "options": ["Invariant risk minimization", "Causal representation learning", "Structural causal models with neural networks", "Counterfactual data augmentation"], "answer": "Invariant risk minimization", "difficulty": "hard" },
{ "id": 223, "question": "Evaluate the design of a neural network optimizer that adapts its update rule based on the local geometry of the loss landscape. Which geometric property is most informative?", "options": ["Hessian eigenvalue distribution", "Gradient noise scale", "Loss surface curvature", "Parameter manifold structure"], "answer": "Parameter manifold structure", "difficulty": "hard" },
{ "id": 224, "question": "Design a neural architecture for lifelong learning that can continuously acquire new knowledge without forgetting previous tasks. Which memory consolidation strategy is most biologically plausible and effective?", "options": ["Synaptic consolidation with homeostatic plasticity", "Complementary learning with hippocampal-neocortical interaction", "Elastic weight consolidation with Fisher information", "Progressive networks with task-specific modules"], "answer": "Complementary learning with hippocampal-neocortical interaction", "difficulty": "hard" },
{ "id": 225, "question": "Analyze the theoretical limits of neural network expressivity for classification tasks. Consider the trade-off between approximation and estimation errors. Which scaling relationship is most accurate?", "options": ["Approximation error dominates in overparameterized regime", "Estimation error scales with sqrt(parameters/samples)", "Optimal complexity balances both errors", "Deep networks always reduce approximation error"], "answer": "Optimal complexity balances both errors", "difficulty": "hard" },
{ "id": 226, "question": "Design a training procedure for neural networks that can handle non-stationary data distributions where the underlying data generating process changes over time. Which adaptation mechanism is most principled?", "options": ["Online learning with forgetting factors", "Change detection with model retraining", "Continual learning with memory consolidation", "Meta-learning with rapid adaptation"], "answer": "Meta-learning with rapid adaptation", "difficulty": "hard" },
{ "id": 227, "question": "Evaluate the design of a neural architecture that incorporates structured prior knowledge about the classification problem (e.g., known invariances, hierarchies). Which knowledge integration approach is most effective?", "options": ["Hard-coded architectural constraints", "Soft regularization with prior-based penalties", "Bayesian neural networks with informative priors", "Physics-informed neural networks with constraint satisfaction"], "answer": "Physics-informed neural networks with constraint satisfaction", "difficulty": "hard" },
{ "id": 228, "question": "Design a neural network that can perform classification while providing guarantees about its predictions (e.g., confidence intervals, coverage guarantees). Which uncertainty quantification framework is most rigorous?", "options": ["Conformal prediction with exchangeability", "Bayesian deep learning with posterior sampling", "Frequentist confidence regions", "PAC-Bayes generalization bounds"], "answer": "Conformal prediction with exchangeability", "difficulty": "hard" },
{ "id": 229, "question": "Analyze the computational and statistical trade-offs in neural architecture design for classification. Consider inference time, memory usage, and accuracy. Which optimization formulation captures all constraints?", "options": ["Multi-objective Pareto optimization", "Constrained optimization with accuracy constraints", "Lagrangian formulation with penalty methods", "Evolutionary multi-objective algorithms"], "answer": "Lagrangian formulation with penalty methods", "difficulty": "hard" },
{ "id": 230, "question": "Design the ultimate neural network training algorithm that combines insights from optimization theory, statistical learning theory, and neuroscience. Which integration principle would lead to the most significant advancement?", "options": ["Biologically-inspired plasticity rules with theoretical guarantees", "Meta-learning algorithms that learn to optimize", "Quantum-inspired optimization with superposition states", "Information-theoretic optimization with minimum description length"], "answer": "Information-theoretic optimization with minimum description length", "difficulty": "hard" }
]

